# -*- coding: utf-8 -*-
"""
Notion é‡è¤‡ãƒšãƒ¼ã‚¸åˆä½“ãƒ„ãƒ¼ãƒ«
- åŒã˜ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒšãƒ¼ã‚¸ãŒè¤‡æ•°ã‚ã‚‹å ´åˆã€ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆä½“ã—ã¦1ã¤ã®ãƒšãƒ¼ã‚¸ã«ã¾ã¨ã‚ã‚‹
- å®‰å…¨ã®ãŸã‚ã€DRY_RUN ãƒ¢ãƒ¼ãƒ‰ã§äº‹å‰ç¢ºèªå¯èƒ½
"""

import os, sys, time, random
from datetime import datetime
from typing import Optional, Dict, Any, List
from collections import defaultdict

try:
    from notion_client import Client
    from notion_client.errors import APIResponseError, RequestTimeoutError
except Exception:
    print("notion-client ãŒã‚ã‚Šã¾ã›ã‚“ã€‚`pip install -r requirements.txt` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)

from dotenv import load_dotenv
load_dotenv()

NOTION_TOKEN = os.getenv("NOTION_TOKEN")
JOURNAL_DB_ID = os.getenv("JOURNAL_DB_ID")
DRY_RUN = os.getenv("DRY_RUN", "true").lower() in ("1", "true", "yes")  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯DRY_RUN
NOTION_TIMEOUT = int(os.getenv("NOTION_TIMEOUT", "60"))

if not NOTION_TOKEN or not JOURNAL_DB_ID:
    print("ç’°å¢ƒå¤‰æ•° NOTION_TOKEN / JOURNAL_DB_ID ãŒæœªè¨­å®šã§ã™ã€‚.env ã‚’ç¢ºèªã€‚")
    sys.exit(1)

# Notion ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
notion = Client(auth=NOTION_TOKEN, timeout_ms=int(NOTION_TIMEOUT * 1000))

# ---------- ãƒªãƒˆãƒ©ã‚¤ä»˜ãAPIå‘¼ã³å‡ºã—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ----------
def with_retry(fn, *, max_attempts=4, base_delay=1.0, what="api"):
    attempt = 0
    while True:
        try:
            return fn()
        except RequestTimeoutError as e:
            attempt += 1
            if attempt >= max_attempts:
                raise
            delay = base_delay * (2 ** (attempt-1)) + random.uniform(0, 0.5)
            print(f"[RETRY] Timeout on {what}. retry {attempt}/{max_attempts} in {delay:.1f}s")
            time.sleep(delay)
        except APIResponseError as e:
            if getattr(e, "code", "") == "rate_limited":
                attempt += 1
                retry_after = float(getattr(e, "headers", {}).get("Retry-After", 1)) if hasattr(e, "headers") else 1.0
                delay = max(retry_after, base_delay * (2 ** (attempt-1))) + random.uniform(0, 0.5)
                print(f"[RETRY] rate_limited on {what}. retry {attempt}/{max_attempts} in {delay:.1f}s")
                time.sleep(delay)
                if attempt < max_attempts:
                    continue
            raise

# ---------- ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œ ----------
def iter_database_pages(database_id: str):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®å…¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
    start_cursor = None
    while True:
        def _call():
            payload = {"database_id": database_id, "page_size": 100}
            if start_cursor:
                payload["start_cursor"] = start_cursor
            return notion.databases.query(**payload)
        res = with_retry(_call, what="databases.query")
        for r in res.get("results", []):
            yield r
        if not res.get("has_more"):
            break
        start_cursor = res.get("next_cursor")

def get_page_title(page: Dict[str, Any]) -> Optional[str]:
    """ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—"""
    props = page.get("properties", {})
    title_prop = props.get("ã‚¿ã‚¤ãƒˆãƒ«")
    if not title_prop:
        return None
    
    title_content = title_prop.get("title", [])
    if not title_content:
        return None
    
    return "".join([span.get("plain_text", "") for span in title_content])

def has_content_data(page: Dict[str, Any]) -> bool:
    """ãƒšãƒ¼ã‚¸ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    props = page.get("properties", {})
    
    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®ãƒªã‚¹ãƒˆ
    relation_props = [
        "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ", "Technique", "è³¼å…¥ã‚¢ã‚¤ãƒ†ãƒ ", "è¨˜äº‹ã‚¢ã‚¤ãƒ‡ã‚¢ä¸€è¦§", "ãƒã‚¤ãƒªãƒ³ã‚¯",
        "æ˜ ç”»ãƒ»ãƒ‰ãƒ©ãƒè¦–è´è¨˜éŒ²", "UberEatsé…é”è¨˜éŒ²", "Clipping", "ğŸŸ¥æ”¯æ‰•äºˆå®š",
        "å…¬é–‹è¨˜äº‹ä¸€è¦§", "ğŸŸ©æ”¯æ‰•å®Œäº†", "DB_è¡Œå‹•", "AI Chatç®¡ç†", "YouTubeè¦ç´„"
    ]
    
    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    for prop_name in relation_props:
        if prop_name in props:
            prop = props[prop_name]
            if prop.get("type") == "relation":
                relations = prop.get("relation", [])
                if relations:
                    return True
    
    # URLãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®ç¢ºèª
    url_prop = props.get("URL")
    if url_prop and url_prop.get("url"):
        return True
    
    # æœ¬æ–‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç¢ºèª
    try:
        def _call():
            return notion.blocks.children.list(block_id=page["id"])
        children = with_retry(_call, what="blocks.children.list")
        
        if children.get("results"):
            return True
    except Exception as e:
        print(f"  è­¦å‘Š: ãƒšãƒ¼ã‚¸ {page['id']} ã®å­ãƒ–ãƒ­ãƒƒã‚¯å–å¾—ã«å¤±æ•—: {e}")
    
    return False

def get_page_relations(page: Dict[str, Any]) -> Dict[str, List[str]]:
    """ãƒšãƒ¼ã‚¸ã®ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    props = page.get("properties", {})
    relations = {}
    
    relation_props = [
        "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ", "Technique", "è³¼å…¥ã‚¢ã‚¤ãƒ†ãƒ ", "è¨˜äº‹ã‚¢ã‚¤ãƒ‡ã‚¢ä¸€è¦§", "ãƒã‚¤ãƒªãƒ³ã‚¯",
        "æ˜ ç”»ãƒ»ãƒ‰ãƒ©ãƒè¦–è´è¨˜éŒ²", "UberEatsé…é”è¨˜éŒ²", "Clipping", "ğŸŸ¥æ”¯æ‰•äºˆå®š",
        "å…¬é–‹è¨˜äº‹ä¸€è¦§", "ğŸŸ©æ”¯æ‰•å®Œäº†", "DB_è¡Œå‹•", "AI Chatç®¡ç†", "YouTubeè¦ç´„"
    ]
    
    for prop_name in relation_props:
        if prop_name in props:
            prop = props[prop_name]
            if prop.get("type") == "relation":
                relation_list = prop.get("relation", [])
                if relation_list:
                    relations[prop_name] = [r["id"] for r in relation_list]
    
    return relations

def get_page_url(page: Dict[str, Any]) -> Optional[str]:
    """ãƒšãƒ¼ã‚¸ã®URLãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’å–å¾—"""
    props = page.get("properties", {})
    url_prop = props.get("URL")
    if url_prop and url_prop.get("url"):
        return url_prop.get("url")
    return None

def merge_page_properties(pages: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è¤‡æ•°ãƒšãƒ¼ã‚¸ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’åˆä½“"""
    merged_props = {}
    
    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®åˆä½“
    all_relations = defaultdict(set)
    merged_url = None
    
    for page in pages:
        props = page.get("properties", {})
        
        # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆä½“
        for prop_name, relation_ids in get_page_relations(page).items():
            all_relations[prop_name].update(relation_ids)
        
        # URLã®å–å¾—ï¼ˆæœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã‚’ä½¿ç”¨ï¼‰
        if not merged_url:
            merged_url = get_page_url(page)
    
    # ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’è¨­å®š
    for prop_name, relation_ids in all_relations.items():
        merged_props[prop_name] = {
            "relation": [{"id": rid} for rid in relation_ids]
        }
    
    # URLãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’è¨­å®š
    if merged_url:
        merged_props["URL"] = {
            "url": merged_url
        }
    
    return merged_props

def update_page_properties(page_id: str, properties: Dict[str, Any]):
    """ãƒšãƒ¼ã‚¸ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’æ›´æ–°"""
    if DRY_RUN:
        print(f"[DRY-RUN] Update page {page_id} properties")
        return
    
    def _call():
        return notion.pages.update(
            page_id=page_id,
            properties=properties
        )
    return with_retry(_call, what="pages.update")

def copy_page_content(source_page_id: str, target_page_id: str):
    """ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚³ãƒ”ãƒ¼"""
    if DRY_RUN:
        print(f"[DRY-RUN] Copy content from {source_page_id} to {target_page_id}")
        return
    
    try:
        # ã‚½ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã®å­ãƒ–ãƒ­ãƒƒã‚¯ã‚’å–å¾—
        def _call():
            return notion.blocks.children.list(block_id=source_page_id)
        children = with_retry(_call, what="blocks.children.list")
        
        if children.get("results"):
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒšãƒ¼ã‚¸ã«å­ãƒ–ãƒ­ãƒƒã‚¯ã‚’è¿½åŠ 
            def _call2():
                return notion.blocks.children.append(
                    block_id=target_page_id,
                    children=children["results"]
                )
            with_retry(_call2, what="blocks.children.append")
    except Exception as e:
        print(f"  è­¦å‘Š: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚³ãƒ”ãƒ¼ã«å¤±æ•—: {e}")

def delete_page(page_id: str):
    """ãƒšãƒ¼ã‚¸ã‚’å‰Šé™¤ï¼ˆã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ï¼‰"""
    if DRY_RUN:
        print(f"[DRY-RUN] Delete page {page_id}")
        return
    
    def _call():
        return notion.pages.update(
            page_id=page_id,
            archived=True
        )
    return with_retry(_call, what="pages.delete")

# ---------- åˆä½“ãƒ­ã‚¸ãƒƒã‚¯ ----------
def find_mergeable_duplicates(pages: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """åˆä½“å¯èƒ½ãªé‡è¤‡ãƒšãƒ¼ã‚¸ã‚’æ¤œå‡ºï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ãƒšãƒ¼ã‚¸ã®ã¿ï¼‰"""
    title_groups = defaultdict(list)
    
    for page in pages:
        title = get_page_title(page)
        if title and has_content_data(page):  # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ãƒšãƒ¼ã‚¸ã®ã¿
            title_groups[title].append(page)
    
    # é‡è¤‡ãŒã‚ã‚Šã€ã‹ã¤è¤‡æ•°ã®ãƒšãƒ¼ã‚¸ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒã‚ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—ã®ã¿ã‚’è¿”ã™
    return {title: pages for title, pages in title_groups.items() 
            if len(pages) > 1 and len([p for p in pages if has_content_data(p)]) > 1}

def select_merge_target(duplicate_pages: List[Dict[str, Any]]) -> Dict[str, Any]:
    """åˆä½“å…ˆã®ãƒšãƒ¼ã‚¸ã‚’é¸æŠï¼ˆæœ€åˆã®ãƒšãƒ¼ã‚¸ã‚’é¸æŠï¼‰"""
    return duplicate_pages[0]

# ---------- ãƒ¡ã‚¤ãƒ³å‡¦ç† ----------
def main():
    print("== Notion é‡è¤‡ãƒšãƒ¼ã‚¸åˆä½“ãƒ„ãƒ¼ãƒ« ==")
    print(f"DRY_RUN: {DRY_RUN}")
    print(f"å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {JOURNAL_DB_ID}")
    print()
    
    # å…¨ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    print("ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
    pages = list(iter_database_pages(JOURNAL_DB_ID))
    if not pages:
        print("å¯¾è±¡ãƒšãƒ¼ã‚¸ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    print(f"ç·ãƒšãƒ¼ã‚¸æ•°: {len(pages)}")
    print()
    
    # åˆä½“å¯èƒ½ãªé‡è¤‡ã‚’æ¤œå‡º
    print("åˆä½“å¯èƒ½ãªé‡è¤‡ãƒšãƒ¼ã‚¸ã‚’æ¤œå‡ºä¸­...")
    mergeable_duplicates = find_mergeable_duplicates(pages)
    
    if not mergeable_duplicates:
        print("åˆä½“å¯èƒ½ãªé‡è¤‡ãƒšãƒ¼ã‚¸ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    print(f"åˆä½“å¯¾è±¡ã‚¿ã‚¤ãƒˆãƒ«æ•°: {len(mergeable_duplicates)}")
    print()
    
    # åˆä½“å‡¦ç†ã®æº–å‚™
    merge_operations = []
    
    for title, duplicate_pages in mergeable_duplicates.items():
        print(f"ã‚¿ã‚¤ãƒˆãƒ«: {title}")
        print(f"  é‡è¤‡ãƒšãƒ¼ã‚¸æ•°: {len(duplicate_pages)}")
        
        # å„ãƒšãƒ¼ã‚¸ã®è©³ç´°ç¢ºèª
        for i, page in enumerate(duplicate_pages):
            page_id = page["id"]
            relations = get_page_relations(page)
            url = get_page_url(page)
            has_content = has_content_data(page)
            
            print(f"    ãƒšãƒ¼ã‚¸{i+1}: {page_id}")
            if relations:
                for rel_name, rel_ids in relations.items():
                    print(f"      ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€Œ{rel_name}ã€: {len(rel_ids)}ä»¶")
            else:
                print(f"      ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: ãªã—")
            print(f"      URL: {'ã‚ã‚Š' if url else 'ãªã—'}")
            print(f"      ã‚³ãƒ³ãƒ†ãƒ³ãƒ„: {'ã‚ã‚Š' if has_content else 'ãªã—'}")
        
        # åˆä½“å…ˆã‚’é¸æŠ
        target_page = select_merge_target(duplicate_pages)
        source_pages = [p for p in duplicate_pages if p["id"] != target_page["id"]]
        
        # åˆä½“å¾Œã®äºˆæƒ³çµæœã‚’è¡¨ç¤º
        print(f"  â†’ åˆä½“å¾Œã®äºˆæƒ³çµæœ:")
        all_relations = defaultdict(set)
        merged_url = None
        
        for page in [target_page] + source_pages:
            for prop_name, relation_ids in get_page_relations(page).items():
                all_relations[prop_name].update(relation_ids)
            if not merged_url:
                merged_url = get_page_url(page)
        
        print(f"    åˆä½“å…ˆ: {target_page['id']}")
        if all_relations:
            for rel_name, rel_ids in all_relations.items():
                print(f"      ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€Œ{rel_name}ã€: {len(rel_ids)}ä»¶")
        if merged_url:
            print(f"      URL: ã‚ã‚Š")
        print(f"      åˆä½“å…ƒ: {len(source_pages)}ä»¶")
        
        merge_operations.append({
            "title": title,
            "target_page": target_page,
            "source_pages": source_pages
        })
        
        print()
    
    print(f"=== çµ±è¨ˆæƒ…å ± ===")
    print(f"ç·ãƒšãƒ¼ã‚¸æ•°: {len(pages)}")
    print(f"åˆä½“å¯¾è±¡ã‚¿ã‚¤ãƒˆãƒ«æ•°: {len(mergeable_duplicates)}")
    print(f"åˆä½“æ“ä½œæ•°: {len(merge_operations)}")
    
    # åˆä½“å¯¾è±¡ã®è©³ç´°çµ±è¨ˆ
    total_source_pages = sum(len(op["source_pages"]) for op in merge_operations)
    total_relations = defaultdict(int)
    
    for operation in merge_operations:
        for page in [operation["target_page"]] + operation["source_pages"]:
            for prop_name, relation_ids in get_page_relations(page).items():
                total_relations[prop_name] += len(relation_ids)
    
    print(f"åˆä½“å…ƒãƒšãƒ¼ã‚¸æ•°: {total_source_pages}")
    print(f"åˆä½“å¾Œã®ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°:")
    for rel_name, count in total_relations.items():
        print(f"  {rel_name}: {count}ä»¶")
    print()
    
    if DRY_RUN:
        print("DRY_RUN ãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã€å®Ÿéš›ã®åˆä½“ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚")
        print("å®Ÿéš›ã«åˆä½“ã™ã‚‹ã«ã¯ã€.env ã§ DRY_RUN=false ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        return
    
    # å®Ÿéš›ã®åˆä½“å®Ÿè¡Œ
    print("å®Ÿéš›ã®åˆä½“ã‚’é–‹å§‹ã—ã¾ã™...")
    confirm = input("ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
    if confirm.lower() != 'y':
        print("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚")
        return
    
    print(f"\n=== åˆä½“ã®å®Ÿè¡Œ ({len(merge_operations)}ä»¶) ===")
    for i, operation in enumerate(merge_operations, 1):
        title = operation["title"]
        target_page = operation["target_page"]
        source_pages = operation["source_pages"]
        
        print(f"[{i}/{len(merge_operations)}] åˆä½“: {title}")
        print(f"  åˆä½“å…ˆ: {target_page['id']}")
        
        # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã®åˆä½“
        merged_props = merge_page_properties([target_page] + source_pages)
        if merged_props:
            print(f"  ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£æ›´æ–°: {list(merged_props.keys())}")
            update_page_properties(target_page["id"], merged_props)
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚³ãƒ”ãƒ¼
        content_copied = 0
        for source_page in source_pages:
            print(f"  ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚³ãƒ”ãƒ¼: {source_page['id']} â†’ {target_page['id']}")
            copy_page_content(source_page["id"], target_page["id"])
            content_copied += 1
        
        # ã‚½ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã®å‰Šé™¤
        for source_page in source_pages:
            print(f"  å‰Šé™¤: {source_page['id']}")
            delete_page(source_page["id"])
        
        print(f"  å®Œäº†: {len(source_pages)}ä»¶ã®ãƒšãƒ¼ã‚¸ã‚’åˆä½“")
        time.sleep(0.5)  # APIåˆ¶é™å¯¾ç­–
        print()
    
    print("å®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nä¸­æ–­ã—ã¾ã—ãŸã€‚")
